<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Please use einsum and einops | Marcus Teller</title> <meta name="author" content="Marcus Teller"/> <meta name="description" content="A short post about why einsum and einops is a great idea to use in deep learning"/> <meta name="keywords" content="diffusion, teller, math, Stochastic"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🎨</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://frozaken.github.io/blog/2022/einoperations/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Marcus </span>Teller</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Please use einsum and einops</h1> <p class="post-meta">October 17, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/pytorch"> <i class="fas fa-hashtag fa-sm"></i> pytorch</a>   <a href="/blog/tag/python"> <i class="fas fa-hashtag fa-sm"></i> python</a>   <a href="/blog/tag/deep-learning"> <i class="fas fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/einops"> <i class="fas fa-hashtag fa-sm"></i> einops</a>     ·   <a href="/blog/category/pytorch"> <i class="fas fa-tag fa-sm"></i> pytorch</a>   </p> </header> <article class="post-content"> <p>In this post I will go over one of the most readable ways of performing tensor algebra in many of the modern deep learning libraries; einstein notation.</p> <h2 id="a-short-motivator">A short motivator</h2> <p>If you ever tried to implement an attention mechanism <a class="citation" href="#attention">[1]</a>, you may have done something like this; Assume \(Q,K\in \mathbb{R}^{n\times d}\) ie, both are sequences of size \(n\), with tokens embedded in \(\mathbb{R}^d\). To calculate the attention distributions, we perform the simple operation</p> \[\mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)\] <p>This is easy to implement; most end up with something like this</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>

<span class="n">dist</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span><span class="o">@</span><span class="n">K</span><span class="p">.</span><span class="n">T</span><span class="o">/</span><span class="n">d</span><span class="p">.</span><span class="n">sqrt</span><span class="p">()).</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code></pre></figure> <p>The question is whether we are happy with this implementation. I would say no, most modern deep learning researchers will apply batches of sequences for parallel processing, instead of one at a time. This means that we in practice assume \(Q,K\in \mathbb{R}^{b\times n\times d}\) where \(b\) is the batch size. Suddenly the distributions are not so easy to calculate without sacrificing readability.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="n">b</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="n">b</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>

<span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">Q</span><span class="o">/</span><span class="n">d</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(),</span> <span class="n">K</span><span class="p">.</span><span class="n">permute</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))).</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></code></pre></figure> <p>If you simply glance over this code, it is not nearly as obvious that this is a batched matrix multiplication where the second operand is transposed. Atleast not nearly as obvious as the non-batched <code class="language-plaintext highlighter-rouge">Q@K.T</code>. The einsum operations deals with this problem; allowing for flexible and complicated tensor computations, whilst maintaining a high degree of readability. Using einsum, this becomes much more readable</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="n">b</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="n">b</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>

<span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'bid,bjd-&gt;bij'</span><span class="p">,</span> <span class="n">Q</span><span class="o">/</span><span class="n">d</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(),</span> <span class="n">K</span><span class="p">).</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></code></pre></figure> <h2 id="a-simple-tldr-of-einsum">A simple TL;DR of einsum</h2> <p>I understand how if you have never seen einsums before, the third example might seem even less readable than the first two. I hope that by the end of this post, you will begin to see the potential of expressing everything using this simple interface. To understand how einsum works, consider first the simple operation of matrix multiplication between matrices \(A,B\) where \(AB=C\), by definition we have</p> \[\sum_k A_{ik}B_{kj} = C_{ij}\] <p>Notice here that the indices for \(A\) is <code class="language-plaintext highlighter-rouge">ik</code>, the indices for \(B\) is <code class="language-plaintext highlighter-rouge">kj</code>, and the output indices are <code class="language-plaintext highlighter-rouge">ij</code>. It turns out that these indices (as long as the follow certain rules), is enough to completely specify the operation. The principle of einsum, is to first specify the indices of the operands, followed by an arrow and the indices of the result. Thus to calculate \(C=AB\), we would simply list the indices we just derived <code class="language-plaintext highlighter-rouge">torch.einsum('ik,kj-&gt;ij',A,B)</code>. Notice then how easily a transpose can be added. Since a transposition is simply a reversal of indices, then one can calculate \(AB^\top\) by simply writing <code class="language-plaintext highlighter-rouge">jk</code> instead of <code class="language-plaintext highlighter-rouge">kj</code> as indexing \(B\) with indices <code class="language-plaintext highlighter-rouge">jk</code> is the same as indexing \(B^\top\) with indices <code class="language-plaintext highlighter-rouge">kj</code>. Thus our transposed matrix product is given by</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'ik,jk-&gt;ij'</span><span class="p">,</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">)</span></code></pre></figure> <p>Notice how suddenly all operations that relate to the order of the dimensions in our tensors became superfluous since we can just specify the indices in a different order. If we go back to the example of attention, we saw how we without batching can perform attention as the soft-max over a simple transposed matrix product <code class="language-plaintext highlighter-rouge">Q@K.T</code>. Lets use the expression we just derived <code class="language-plaintext highlighter-rouge">torch.einsum('id,jd-&gt;ij',Q/d.sqrt(),K)</code>, and instead of using <code class="language-plaintext highlighter-rouge">k</code> as an index, we will use <code class="language-plaintext highlighter-rouge">d</code>, to remind ourselves that we are summing over the embedding dimension. The big revelation is now that turning this into a batch operation is extremely simple. All we have to do, is add a batch index we do not sum over</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'bid,bjd-&gt;bij'</span><span class="p">,</span><span class="n">Q</span><span class="o">/</span><span class="n">d</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(),</span><span class="n">K</span><span class="p">)</span></code></pre></figure> <p>It seems almost magical (for those used to batching more complicated operations), that we can turn these matrix operations into batched operations by simply adding an index to the computation.</p> <p>We can also specify an einsum with does not sum over anything. The simplest of these operations is the transpose itself. by specifying a reversal of indices, we get a transpose as we would expect</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'ij-&gt;ji'</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span></code></pre></figure> <p>We can also reduce all dimensions, in which case we get a simple sum over the entire tensor.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'ij-&gt;'</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span></code></pre></figure> <h2 id="why-you-need-einops">Why you need einops</h2> <p>There are some limitations with this however, we cannot really perform operations that significantly changes the shape, such as taking a dataset with \(N=K\cdot b\) samples, and splitting it into \(K\) batches. This is where the python library <a href="https://einops.rocks/" target="_blank" rel="noopener noreferrer">einops</a> comes in handy. It essentially extends the classical einsum, into a much more flexible tool to manipulate shapes.</p> <p>Consider the problem of having a batch of size \(b\) containing RBG images of size \(128\times128\). A common operation <a class="citation" href="#vit">[2]</a> is to cut this image into patches of size \(h\times w\). One of the first results that pop up on google when searching this, is <a href="https://discuss.pytorch.org/t/slicing-image-into-square-patches/8772/2" target="_blank" rel="noopener noreferrer">this discussion</a>, which run the following (completely unreadable) code</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">patches</span> <span class="o">=</span> <span class="n">img_t</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">unfold</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">).</span><span class="n">unfold</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span></code></pre></figure> <p>Lets see if we can do better. The thought process needed to write einops is very similar to einsum. The one difference I would point out after using both for quite some time, is that einops thinks more in equations between shapes, rather than indices explicitly as in einsums. Lets try and cut out some \(4\times 8\) patches from these images. First, we realize that the tensors we are dealing with have shape <code class="language-plaintext highlighter-rouge">(b, 128, 128, 3)</code>, and we would like them to have shape <code class="language-plaintext highlighter-rouge">(b, 512, 4, 8, 3)</code>. We can write this as an equation. We denote the number of patches in horizontal direction <code class="language-plaintext highlighter-rouge">nw</code> and the number in the vertical direction <code class="language-plaintext highlighter-rouge">nh</code>. We can then write the original shape as <code class="language-plaintext highlighter-rouge">(b, nh*4, nw*8, 3)</code>, and the resulting shape as <code class="language-plaintext highlighter-rouge">(b, nh*nw, 4, 8, 3)</code>. This is exactly what we do with the rearrange operation from einops</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">einops</span> <span class="kn">import</span> <span class="n">rearrange</span>

<span class="n">patches</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">ims</span><span class="p">,</span> <span class="s">'b (nh h) (nw w) c -&gt; b (nh nw) h w c'</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">8</span><span class="p">)</span></code></pre></figure> <p>This will result in a tensor where <code class="language-plaintext highlighter-rouge">patches[0,0]==ims[0, :4, :8]</code>, which is exactly what we were looking for. Notice here than instead of the explicit multiplication <code class="language-plaintext highlighter-rouge">nh*4</code>, einops uses the notation <code class="language-plaintext highlighter-rouge">(n k)</code> to specify the multiplication of <code class="language-plaintext highlighter-rouge">n</code> and <code class="language-plaintext highlighter-rouge">k</code>. Notice how we also parameterize the entire operation by arguments to rearrange. Now we can easily dynamically change the patch size we want to extract, by using the two free variables <code class="language-plaintext highlighter-rouge">h</code> and <code class="language-plaintext highlighter-rouge">w</code> in the einops equation. It is also worth noting that einops enforces space between each variables in the equation unlike einsum. This allows for multicharacter variables, but is slightly less compact.</p> <p>Another common operation that I have seen countless deep learning repositories perform, is changing a batch of images from channel-last to channel-first. Rearrange is also perfect for this, as it gives the very readable equation</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">rearrange</span><span class="p">(</span><span class="n">ims</span><span class="p">,</span> <span class="s">'b h w c -&gt; b c h w'</span><span class="p">)</span></code></pre></figure> <p>Notice how we do not even need to know the signature of <code class="language-plaintext highlighter-rouge">ims</code>, as it is listed very explicitly on the left hand side of the equation <code class="language-plaintext highlighter-rouge">b h w c</code>. This is the major advantage of einops in my opinion, it is very easy to glance over a set of operations, and have a fairly good idea of how the shapes of tensors change throught the network.</p> <p>What is interesting for us, is that einops actually lets us define these operations before we even have an operand. For many people trying to work with CNN’s for the first time, they include a flattening operation after their last convolution like so</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Model</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">...)</span>
        <span class="p">...</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="p">...</span></code></pre></figure> <p>Whilst this definetly does the job, it does not say much about what we are actually flattening. Einops makes this much more readable, by letting us specify the operation upfront</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">einops.layers.torch</span> <span class="kn">import</span> <span class="n">Rearrange</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">...)</span>
        <span class="p">...</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">Rearrange</span><span class="p">(</span><span class="s">'b c h w -&gt; b (c h w)'</span><span class="p">)</span>
        <span class="p">...</span></code></pre></figure> <p>There is certainly overhead in this implementation for a newcomer, as the <code class="language-plaintext highlighter-rouge">rearrange</code> operation requires much more active thought about how your shapes change. With the flatten operation, you do not really care much about the input nor the output shape. This does lead to more bugs than the <code class="language-plaintext highlighter-rouge">rearrange</code> operation, since giving it any tensor with a different signature than the left hand side of the equation will throw an error. As deep learning engineers, I think we really should appreciate anything that throws an error - silent errors are very common in DL, and I believe newcomers aswell as veterans should get used to being very aware of their shapes.</p> <p>If this short teaser sparked your curiosity, then i highly reccomend you check out <a href="https://einops.rocks/" target="_blank" rel="noopener noreferrer">einops</a>. They have many more use cases (and operations) than the ones covered here.</p> <h2 id="references">References</h2> <ol class="bibliography"> <li><span id="attention">[1]A. Vaswani <i>et al.</i>, “Attention Is All You Need,” <i>CoRR</i>, vol. abs/1706.03762, 2017 [Online]. Available at: http://arxiv.org/abs/1706.03762</span></li> <li><span id="vit">[2]A. Dosovitskiy <i>et al.</i>, “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,” <i>CoRR</i>, vol. abs/2010.11929, 2020 [Online]. Available at: https://arxiv.org/abs/2010.11929</span></li> </ol> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2022 Marcus Teller. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>